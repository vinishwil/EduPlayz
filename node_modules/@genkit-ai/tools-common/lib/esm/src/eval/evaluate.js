import { randomUUID } from 'crypto';
import { z } from 'zod';
import { getDatasetStore, getEvalStore } from '.';
import { DatasetSchema, GenerateActionOptionsSchema, GenerateResponseSchema, } from '../types';
import { evaluatorName, generateTestCaseId, getEvalExtractors, getModelInput, hasAction, isEvaluator, logger, stackTraceSpans, } from '../utils';
import { enrichResultsWithScoring, extractMetricSummaries, extractMetricsMetadata, } from './parser';
const SUPPORTED_ACTION_TYPES = ['flow', 'model', 'executable-prompt'];
const GENERATE_ACTION_UTIL = '/util/generate';
export async function runNewEvaluation(manager, request) {
    const { dataSource, actionRef, evaluators } = request;
    const { datasetId, data } = dataSource;
    if (!datasetId && !data) {
        throw new Error(`Either 'data' or 'datasetId' must be provided`);
    }
    const hasTargetAction = await hasAction({ manager, actionRef });
    if (!hasTargetAction) {
        throw new Error(`Cannot find action ${actionRef}.`);
    }
    let inferenceDataset;
    let metadata = {};
    if (datasetId) {
        const datasetStore = await getDatasetStore();
        logger.info(`Fetching dataset ${datasetId}...`);
        const dataset = await datasetStore.getDataset(datasetId);
        if (dataset.length === 0) {
            throw new Error(`Dataset ${datasetId} is empty`);
        }
        inferenceDataset = DatasetSchema.parse(dataset);
        const datasetMetadatas = await datasetStore.listDatasets();
        const targetDatasetMetadata = datasetMetadatas.find((d) => d.datasetId === datasetId);
        const datasetVersion = targetDatasetMetadata?.version;
        metadata = { datasetId, datasetVersion };
    }
    else {
        const rawData = data.map((sample) => ({
            ...sample,
            testCaseId: sample.testCaseId ?? generateTestCaseId(),
        }));
        inferenceDataset = DatasetSchema.parse(rawData);
    }
    logger.info('Running inference...');
    const evalDataset = await runInference({
        manager,
        actionRef,
        inferenceDataset,
        context: request.options?.context,
        actionConfig: request.options?.actionConfig,
    });
    const evaluatorActions = await getMatchingEvaluatorActions(manager, evaluators);
    const evalRun = await runEvaluation({
        manager,
        evaluatorActions,
        evalDataset,
        batchSize: request.options?.batchSize,
        augments: {
            ...metadata,
            actionRef,
            actionConfig: request.options?.actionConfig,
        },
    });
    return evalRun.key;
}
export async function runInference(params) {
    const { manager, actionRef, inferenceDataset, context, actionConfig } = params;
    if (!isSupportedActionRef(actionRef)) {
        throw new Error('Inference is only supported on flows and models');
    }
    const evalDataset = await bulkRunAction({
        manager,
        actionRef,
        inferenceDataset,
        context,
        actionConfig,
    });
    return evalDataset;
}
export async function runEvaluation(params) {
    const { manager, evaluatorActions, evalDataset, augments, batchSize } = params;
    if (evalDataset.length === 0) {
        throw new Error('Cannot run evaluation, no data provided');
    }
    const evalRunId = randomUUID();
    const scores = {};
    logger.info('Running evaluation...');
    const runtime = manager.getMostRecentRuntime();
    const isNodeRuntime = runtime?.genkitVersion?.startsWith('nodejs') ?? false;
    for (const action of evaluatorActions) {
        const name = evaluatorName(action);
        const response = await manager.runAction({
            key: name,
            input: {
                dataset: evalDataset.filter((row) => !row.error),
                evalRunId,
                batchSize: isNodeRuntime ? batchSize : undefined,
            },
        });
        scores[name] = response.result;
        logger.info(`Finished evaluator '${action.name}'. Trace ID: ${response.telemetry?.traceId}`);
    }
    const scoredResults = enrichResultsWithScoring(scores, evalDataset);
    const metadata = extractMetricsMetadata(evaluatorActions);
    const metricSummaries = extractMetricSummaries(scores);
    const evalRun = {
        key: {
            evalRunId,
            createdAt: new Date().toISOString(),
            metricSummaries,
            metricsMetadata: metadata,
            ...augments,
        },
        results: scoredResults,
    };
    logger.info('Finished evaluation, writing key...');
    const evalStore = await getEvalStore();
    await evalStore.save(evalRun);
    return evalRun;
}
export async function getAllEvaluatorActions(manager) {
    const allActions = await manager.listActions();
    const allEvaluatorActions = [];
    for (const key in allActions) {
        if (isEvaluator(key)) {
            allEvaluatorActions.push(allActions[key]);
        }
    }
    return allEvaluatorActions;
}
export async function getMatchingEvaluatorActions(manager, evaluators) {
    if (!evaluators) {
        return [];
    }
    const allEvaluatorActions = await getAllEvaluatorActions(manager);
    const filteredEvaluatorActions = allEvaluatorActions.filter((action) => evaluators.includes(action.key));
    if (filteredEvaluatorActions.length === 0) {
        if (allEvaluatorActions.length == 0) {
            throw new Error('No evaluators installed');
        }
    }
    return filteredEvaluatorActions;
}
async function bulkRunAction(params) {
    const { manager, actionRef, inferenceDataset, context, actionConfig } = params;
    const actionType = getSupportedActionType(actionRef);
    if (inferenceDataset.length === 0) {
        throw new Error('Cannot run inference, no data provided');
    }
    const fullInferenceDataset = inferenceDataset;
    const states = [];
    const evalInputs = [];
    for (const sample of fullInferenceDataset) {
        logger.info(`Running inference '${actionRef}' ...`);
        if (actionType === 'model') {
            states.push(await runModelAction({
                manager,
                actionRef,
                sample,
                modelConfig: actionConfig,
            }));
        }
        else if (actionType === 'flow') {
            states.push(await runFlowAction({
                manager,
                actionRef,
                sample,
                context,
            }));
        }
        else {
            states.push(await runPromptAction({
                manager,
                actionRef,
                sample,
                context,
                promptConfig: actionConfig,
            }));
        }
    }
    logger.info(`Gathering evalInputs...`);
    for (const state of states) {
        evalInputs.push(await gatherEvalInput({ manager, actionRef, state }));
    }
    return evalInputs;
}
async function runFlowAction(params) {
    const { manager, actionRef, sample, context } = { ...params };
    let state;
    try {
        const runActionResponse = await manager.runAction({
            key: actionRef,
            input: sample.input,
            context: context ? JSON.parse(context) : undefined,
        });
        state = {
            ...sample,
            traceIds: runActionResponse.telemetry?.traceId
                ? [runActionResponse.telemetry?.traceId]
                : [],
            response: runActionResponse.result,
        };
    }
    catch (e) {
        const traceId = e?.data?.details?.traceId;
        state = {
            ...sample,
            traceIds: traceId ? [traceId] : [],
            evalError: `Error when running inference. Details: ${e?.message ?? e}`,
        };
    }
    return state;
}
async function runModelAction(params) {
    const { manager, actionRef, modelConfig, sample } = { ...params };
    let state;
    try {
        const modelInput = getModelInput(sample.input, modelConfig);
        const runActionResponse = await manager.runAction({
            key: actionRef,
            input: modelInput,
        });
        state = {
            ...sample,
            traceIds: runActionResponse.telemetry?.traceId
                ? [runActionResponse.telemetry?.traceId]
                : [],
            response: runActionResponse.result,
        };
    }
    catch (e) {
        const traceId = e?.data?.details?.traceId;
        state = {
            ...sample,
            traceIds: traceId ? [traceId] : [],
            evalError: `Error when running inference. Details: ${e?.message ?? e}`,
        };
    }
    return state;
}
async function runPromptAction(params) {
    const { manager, actionRef, sample, context, promptConfig } = { ...params };
    const { model: modelFromConfig, ...restOfConfig } = promptConfig;
    if (!modelFromConfig) {
        throw new Error('Missing model: Please specific model for prompt evaluation');
    }
    const model = modelFromConfig.split('/model/').pop();
    if (!model) {
        throw new Error(`Improper model provided: ${modelFromConfig}`);
    }
    let state;
    let renderedPrompt;
    try {
        const runActionResponse = await manager.runAction({
            key: actionRef,
            input: sample.input,
            context: context ? JSON.parse(context) : undefined,
        });
        renderedPrompt = {
            traceId: runActionResponse.telemetry?.traceId,
            result: GenerateActionOptionsSchema.parse(runActionResponse.result),
        };
    }
    catch (e) {
        if (e instanceof z.ZodError) {
            state = {
                ...sample,
                traceIds: [],
                evalError: `Error parsing prompt response. Details: ${JSON.stringify(e.format())}`,
            };
        }
        else {
            const traceId = e?.data?.details?.traceId;
            state = {
                ...sample,
                traceIds: traceId ? [traceId] : [],
                evalError: `Error when rendering prompt. Details: ${e?.message ?? e}`,
            };
        }
        return state;
    }
    try {
        let modelInput = renderedPrompt.result;
        if (Object.keys(restOfConfig ?? {}).length > 0) {
            modelInput = { ...modelInput, model, config: restOfConfig };
        }
        else {
            modelInput = { ...modelInput, model };
        }
        const runActionResponse = await manager.runAction({
            key: GENERATE_ACTION_UTIL,
            input: modelInput,
        });
        const traceIds = runActionResponse.telemetry?.traceId
            ? [renderedPrompt.traceId, runActionResponse.telemetry?.traceId]
            : [renderedPrompt.traceId];
        state = {
            ...sample,
            traceIds: traceIds,
            response: runActionResponse.result,
        };
    }
    catch (e) {
        const traceId = e?.data?.details?.traceId;
        const traceIds = traceId
            ? [renderedPrompt.traceId, traceId]
            : [renderedPrompt.traceId];
        state = {
            ...sample,
            traceIds: traceIds.filter((t) => !!t),
            evalError: `Error when running inference. Details: ${e?.message ?? e}`,
        };
    }
    return state;
}
async function gatherEvalInput(params) {
    const { manager, actionRef, state } = params;
    const actionType = getSupportedActionType(actionRef);
    const extractors = await getEvalExtractors(actionRef);
    const traceIds = state.traceIds;
    if (traceIds.length === 0 ||
        (actionType === 'executable-prompt' && traceIds.length < 2)) {
        logger.warn('No valid traceId available...');
        return {
            ...state,
            error: state.evalError,
            testCaseId: state.testCaseId,
            traceIds: traceIds,
        };
    }
    const traceId = traceIds.at(-1);
    await new Promise((resolve) => setTimeout(resolve, 2000));
    const trace = await manager.getTrace({
        traceId,
    });
    const input = actionType === 'flow' ? extractors.input(trace) : state.input;
    let custom = undefined;
    if (actionType === 'executable-prompt') {
        const promptTrace = await manager.getTrace({
            traceId: traceIds[0],
        });
        custom = { renderedPrompt: extractors.output(promptTrace) };
    }
    const nestedSpan = stackTraceSpans(trace);
    if (!nestedSpan) {
        return {
            testCaseId: state.testCaseId,
            input,
            error: `Unable to extract any spans from trace ${traceId}`,
            reference: state.reference,
            custom,
            traceIds: traceIds,
        };
    }
    if (nestedSpan.attributes['genkit:state'] === 'error') {
        return {
            testCaseId: state.testCaseId,
            input,
            error: getSpanErrorMessage(nestedSpan) ?? `Unknown error in trace ${traceId}`,
            reference: state.reference,
            custom,
            traceIds: traceIds,
        };
    }
    const output = extractors.output(trace);
    const context = extractors.context(trace);
    const error = actionType === 'model' ? getErrorFromModelResponse(output) : undefined;
    return {
        testCaseId: state.testCaseId,
        input,
        output,
        error,
        context: Array.isArray(context) ? context : [context],
        reference: state.reference,
        custom,
        traceIds: traceIds,
    };
}
function getSpanErrorMessage(span) {
    if (span && span.status?.code === 2) {
        const event = span.timeEvents?.timeEvent
            ?.filter((e) => e.annotation.description === 'exception')
            .shift();
        return (event?.annotation?.attributes['exception.message'] ?? 'Error');
    }
}
function getErrorFromModelResponse(obj) {
    const response = GenerateResponseSchema.parse(obj);
    const hasLegacyResponse = !!response.candidates && response.candidates.length > 0;
    const hasNewResponse = !!response.message;
    if (!response || (!hasLegacyResponse && !hasNewResponse)) {
        return `No response was extracted from the output. '${JSON.stringify(obj)}'`;
    }
    if (hasLegacyResponse) {
        const candidate = response.candidates[0];
        if (candidate.finishReason === 'blocked') {
            return candidate.finishMessage || `Generation was blocked by the model.`;
        }
    }
    if (hasNewResponse) {
        if (response.finishReason === 'blocked') {
            return response.finishMessage || `Generation was blocked by the model.`;
        }
    }
}
function isSupportedActionRef(actionRef) {
    return SUPPORTED_ACTION_TYPES.some((supportedType) => actionRef.startsWith(`/${supportedType}`));
}
function getSupportedActionType(actionRef) {
    if (actionRef.startsWith('/model')) {
        return 'model';
    }
    if (actionRef.startsWith('/flow')) {
        return 'flow';
    }
    if (actionRef.startsWith('/executable-prompt')) {
        return 'executable-prompt';
    }
    throw new Error(`Unsupported action type: ${actionRef}`);
}
//# sourceMappingURL=evaluate.js.map